Loading pretrained model
Loading datasets
Training
Trainable parameters: 0.108% (3.473M/3212.750M)
Starting training..., iters: 3000
Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]Calculating loss...:   4%|▍         | 1/25 [00:02<00:53,  2.21s/it]Calculating loss...:   8%|▊         | 2/25 [00:06<01:18,  3.41s/it]Calculating loss...:  12%|█▏        | 3/25 [00:08<01:01,  2.78s/it]Calculating loss...:  16%|█▌        | 4/25 [00:11<01:04,  3.07s/it]Calculating loss...:  20%|██        | 5/25 [00:14<00:56,  2.82s/it]Calculating loss...:  24%|██▍       | 6/25 [00:17<00:54,  2.87s/it]Calculating loss...:  28%|██▊       | 7/25 [00:23<01:09,  3.87s/it]Calculating loss...:  32%|███▏      | 8/25 [00:26<01:03,  3.73s/it]Calculating loss...:  36%|███▌      | 9/25 [00:27<00:45,  2.86s/it]Calculating loss...:  40%|████      | 10/25 [00:30<00:42,  2.81s/it]Calculating loss...:  44%|████▍     | 11/25 [00:34<00:45,  3.23s/it]Calculating loss...:  48%|████▊     | 12/25 [00:38<00:43,  3.37s/it]Calculating loss...:  52%|█████▏    | 13/25 [00:39<00:32,  2.73s/it]Calculating loss...:  56%|█████▌    | 14/25 [00:41<00:26,  2.38s/it]Calculating loss...:  60%|██████    | 15/25 [00:42<00:19,  1.95s/it]Calculating loss...:  64%|██████▍   | 16/25 [00:43<00:16,  1.83s/it]Calculating loss...:  68%|██████▊   | 17/25 [00:50<00:26,  3.25s/it]Calculating loss...:  72%|███████▏  | 18/25 [00:53<00:22,  3.17s/it]Calculating loss...:  76%|███████▌  | 19/25 [00:57<00:20,  3.40s/it]Calculating loss...:  80%|████████  | 20/25 [00:58<00:14,  2.82s/it]Calculating loss...:  84%|████████▍ | 21/25 [01:04<00:14,  3.67s/it]Calculating loss...:  88%|████████▊ | 22/25 [01:05<00:08,  2.99s/it]Calculating loss...:  92%|█████████▏| 23/25 [01:08<00:05,  2.93s/it]Calculating loss...:  96%|█████████▌| 24/25 [01:09<00:02,  2.40s/it]Calculating loss...: 100%|██████████| 25/25 [01:14<00:00,  3.15s/it]Calculating loss...: 100%|██████████| 25/25 [01:14<00:00,  2.98s/it]
Iter 1: Val loss 2.666, Val took 74.453s
Iter 10: Train loss 2.753, Learning Rate 1.000e-04, It/sec 0.248, Tokens/sec 102.938, Trained Tokens 4153, Peak mem 7.916 GB
Iter 20: Train loss 2.533, Learning Rate 1.000e-04, It/sec 0.223, Tokens/sec 104.621, Trained Tokens 8848, Peak mem 7.975 GB
Iter 30: Train loss 2.590, Learning Rate 1.000e-04, It/sec 0.188, Tokens/sec 88.773, Trained Tokens 13565, Peak mem 8.533 GB
Iter 40: Train loss 2.539, Learning Rate 1.000e-04, It/sec 0.287, Tokens/sec 102.197, Trained Tokens 17120, Peak mem 8.533 GB
Iter 50: Train loss 2.442, Learning Rate 1.000e-04, It/sec 0.253, Tokens/sec 114.718, Trained Tokens 21648, Peak mem 8.533 GB
Iter 60: Train loss 2.477, Learning Rate 1.000e-04, It/sec 0.198, Tokens/sec 107.164, Trained Tokens 27056, Peak mem 8.966 GB
Iter 70: Train loss 2.240, Learning Rate 1.000e-04, It/sec 0.214, Tokens/sec 106.447, Trained Tokens 32032, Peak mem 8.966 GB
Iter 80: Train loss 2.444, Learning Rate 1.000e-04, It/sec 0.229, Tokens/sec 103.195, Trained Tokens 36541, Peak mem 8.966 GB
Iter 90: Train loss 2.291, Learning Rate 1.000e-04, It/sec 0.204, Tokens/sec 97.059, Trained Tokens 41306, Peak mem 8.966 GB
Iter 100: Train loss 2.391, Learning Rate 1.000e-04, It/sec 0.190, Tokens/sec 114.109, Trained Tokens 47304, Peak mem 8.966 GB
Iter 110: Train loss 2.251, Learning Rate 1.000e-04, It/sec 0.210, Tokens/sec 113.346, Trained Tokens 52694, Peak mem 8.966 GB
Iter 120: Train loss 2.410, Learning Rate 1.000e-04, It/sec 0.248, Tokens/sec 112.464, Trained Tokens 57227, Peak mem 8.966 GB
Iter 130: Train loss 2.364, Learning Rate 1.000e-04, It/sec 0.233, Tokens/sec 116.025, Trained Tokens 62197, Peak mem 8.966 GB
Iter 140: Train loss 2.281, Learning Rate 1.000e-04, It/sec 0.231, Tokens/sec 114.525, Trained Tokens 67145, Peak mem 8.966 GB
Iter 150: Train loss 2.350, Learning Rate 1.000e-04, It/sec 0.185, Tokens/sec 114.288, Trained Tokens 73322, Peak mem 8.966 GB
Iter 160: Train loss 2.345, Learning Rate 1.000e-04, It/sec 0.290, Tokens/sec 114.259, Trained Tokens 77266, Peak mem 8.966 GB
Iter 170: Train loss 2.270, Learning Rate 1.000e-04, It/sec 0.208, Tokens/sec 115.809, Trained Tokens 82824, Peak mem 8.966 GB
Iter 180: Train loss 2.366, Learning Rate 1.000e-04, It/sec 0.245, Tokens/sec 113.242, Trained Tokens 87442, Peak mem 8.966 GB
Iter 190: Train loss 2.138, Learning Rate 1.000e-04, It/sec 0.242, Tokens/sec 115.256, Trained Tokens 92195, Peak mem 8.966 GB
Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]Calculating loss...:   4%|▍         | 1/25 [00:04<01:36,  4.03s/it]Calculating loss...:   8%|▊         | 2/25 [00:04<00:50,  2.19s/it]Calculating loss...:  12%|█▏        | 3/25 [00:06<00:38,  1.75s/it]Calculating loss...:  16%|█▌        | 4/25 [00:09<00:52,  2.50s/it]Calculating loss...:  20%|██        | 5/25 [00:11<00:44,  2.24s/it]Calculating loss...:  24%|██▍       | 6/25 [00:12<00:35,  1.89s/it]Calculating loss...:  28%|██▊       | 7/25 [00:15<00:39,  2.17s/it]Calculating loss...:  32%|███▏      | 8/25 [00:16<00:31,  1.86s/it]Calculating loss...:  36%|███▌      | 9/25 [00:17<00:24,  1.56s/it]Calculating loss...:  40%|████      | 10/25 [00:18<00:20,  1.37s/it]Calculating loss...:  44%|████▍     | 11/25 [00:19<00:17,  1.24s/it]Calculating loss...:  48%|████▊     | 12/25 [00:20<00:13,  1.04s/it]Calculating loss...:  52%|█████▏    | 13/25 [00:22<00:18,  1.55s/it]Calculating loss...:  56%|█████▌    | 14/25 [00:26<00:22,  2.09s/it]Calculating loss...:  60%|██████    | 15/25 [00:29<00:23,  2.36s/it]Calculating loss...:  64%|██████▍   | 16/25 [00:32<00:23,  2.66s/it]Calculating loss...:  68%|██████▊   | 17/25 [00:38<00:28,  3.54s/it]Calculating loss...:  72%|███████▏  | 18/25 [00:42<00:26,  3.80s/it]Calculating loss...:  76%|███████▌  | 19/25 [00:45<00:21,  3.57s/it]Calculating loss...:  80%|████████  | 20/25 [00:49<00:18,  3.69s/it]Calculating loss...:  84%|████████▍ | 21/25 [00:53<00:15,  3.80s/it]Calculating loss...:  88%|████████▊ | 22/25 [00:56<00:10,  3.49s/it]Calculating loss...:  92%|█████████▏| 23/25 [01:02<00:08,  4.29s/it]Calculating loss...:  96%|█████████▌| 24/25 [01:04<00:03,  3.64s/it]Calculating loss...: 100%|██████████| 25/25 [01:11<00:00,  4.70s/it]Calculating loss...: 100%|██████████| 25/25 [01:11<00:00,  2.87s/it]
Iter 200: Val loss 2.352, Val took 71.764s
Iter 200: Train loss 2.218, Learning Rate 1.000e-04, It/sec 0.215, Tokens/sec 115.265, Trained Tokens 97555, Peak mem 8.966 GB
Iter 200: Saved adapter weights to adapters/mindmate_llama32_3b_lora/adapters.safetensors and adapters/mindmate_llama32_3b_lora/0000200_adapters.safetensors.
Iter 210: Train loss 2.300, Learning Rate 1.000e-04, It/sec 0.302, Tokens/sec 114.618, Trained Tokens 101350, Peak mem 8.966 GB
Iter 220: Train loss 2.238, Learning Rate 1.000e-04, It/sec 0.226, Tokens/sec 111.923, Trained Tokens 106300, Peak mem 8.966 GB
Iter 230: Train loss 2.373, Learning Rate 1.000e-04, It/sec 0.304, Tokens/sec 111.444, Trained Tokens 109966, Peak mem 8.966 GB
Iter 240: Train loss 2.247, Learning Rate 1.000e-04, It/sec 0.207, Tokens/sec 114.708, Trained Tokens 115516, Peak mem 8.966 GB
Iter 250: Train loss 2.310, Learning Rate 1.000e-04, It/sec 0.261, Tokens/sec 114.267, Trained Tokens 119894, Peak mem 8.966 GB
Iter 260: Train loss 2.265, Learning Rate 1.000e-04, It/sec 0.263, Tokens/sec 116.319, Trained Tokens 124311, Peak mem 8.966 GB
Iter 270: Train loss 2.374, Learning Rate 1.000e-04, It/sec 0.245, Tokens/sec 114.272, Trained Tokens 128975, Peak mem 8.966 GB
Iter 280: Train loss 2.378, Learning Rate 1.000e-04, It/sec 0.251, Tokens/sec 116.601, Trained Tokens 133621, Peak mem 8.966 GB
Iter 290: Train loss 2.291, Learning Rate 1.000e-04, It/sec 0.343, Tokens/sec 115.243, Trained Tokens 136984, Peak mem 8.966 GB
Iter 300: Train loss 2.369, Learning Rate 1.000e-04, It/sec 0.253, Tokens/sec 115.370, Trained Tokens 141545, Peak mem 8.966 GB
Iter 310: Train loss 2.458, Learning Rate 1.000e-04, It/sec 0.250, Tokens/sec 116.607, Trained Tokens 146206, Peak mem 8.966 GB
Iter 320: Train loss 2.184, Learning Rate 1.000e-04, It/sec 0.329, Tokens/sec 114.615, Trained Tokens 149689, Peak mem 8.966 GB
Iter 330: Train loss 2.293, Learning Rate 1.000e-04, It/sec 0.206, Tokens/sec 116.392, Trained Tokens 155340, Peak mem 8.966 GB
Iter 340: Train loss 2.361, Learning Rate 1.000e-04, It/sec 0.251, Tokens/sec 115.575, Trained Tokens 159936, Peak mem 8.966 GB
Iter 350: Train loss 2.399, Learning Rate 1.000e-04, It/sec 0.339, Tokens/sec 112.079, Trained Tokens 163240, Peak mem 8.966 GB
Iter 360: Train loss 2.298, Learning Rate 1.000e-04, It/sec 0.294, Tokens/sec 115.578, Trained Tokens 167166, Peak mem 8.966 GB
Iter 370: Train loss 2.397, Learning Rate 1.000e-04, It/sec 0.183, Tokens/sec 113.566, Trained Tokens 173386, Peak mem 8.966 GB
Iter 380: Train loss 2.247, Learning Rate 1.000e-04, It/sec 0.290, Tokens/sec 117.430, Trained Tokens 177429, Peak mem 8.966 GB
Iter 390: Train loss 2.307, Learning Rate 1.000e-04, It/sec 0.305, Tokens/sec 110.855, Trained Tokens 181060, Peak mem 8.966 GB
Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]Calculating loss...:   4%|▍         | 1/25 [00:03<01:29,  3.73s/it]Calculating loss...:   8%|▊         | 2/25 [00:07<01:20,  3.50s/it]Calculating loss...:  12%|█▏        | 3/25 [00:12<01:31,  4.16s/it]Calculating loss...:  16%|█▌        | 4/25 [00:13<01:05,  3.11s/it]Calculating loss...:  20%|██        | 5/25 [00:15<00:57,  2.85s/it]Calculating loss...:  24%|██▍       | 6/25 [00:16<00:41,  2.20s/it]Calculating loss...:  28%|██▊       | 7/25 [00:18<00:34,  1.90s/it]Calculating loss...:  32%|███▏      | 8/25 [00:20<00:37,  2.19s/it]Calculating loss...:  36%|███▌      | 9/25 [00:24<00:39,  2.47s/it]Calculating loss...:  40%|████      | 10/25 [00:27<00:39,  2.64s/it]Calculating loss...:  44%|████▍     | 11/25 [00:27<00:29,  2.10s/it]Calculating loss...:  48%|████▊     | 12/25 [00:29<00:23,  1.83s/it]Calculating loss...:  52%|█████▏    | 13/25 [00:30<00:18,  1.55s/it]Calculating loss...:  56%|█████▌    | 14/25 [00:32<00:18,  1.72s/it]Calculating loss...:  60%|██████    | 15/25 [00:33<00:14,  1.50s/it]Calculating loss...:  64%|██████▍   | 16/25 [00:37<00:21,  2.35s/it]Calculating loss...:  68%|██████▊   | 17/25 [00:39<00:18,  2.29s/it]Calculating loss...:  72%|███████▏  | 18/25 [00:43<00:19,  2.81s/it]Calculating loss...:  76%|███████▌  | 19/25 [00:47<00:18,  3.07s/it]Calculating loss...:  80%|████████  | 20/25 [00:50<00:16,  3.24s/it]Calculating loss...:  84%|████████▍ | 21/25 [00:52<00:10,  2.72s/it]Calculating loss...:  88%|████████▊ | 22/25 [00:55<00:08,  2.82s/it]Calculating loss...:  92%|█████████▏| 23/25 [00:59<00:06,  3.19s/it]Calculating loss...:  96%|█████████▌| 24/25 [01:00<00:02,  2.51s/it]Calculating loss...: 100%|██████████| 25/25 [01:02<00:00,  2.23s/it]Calculating loss...: 100%|██████████| 25/25 [01:02<00:00,  2.48s/it]
Iter 400: Val loss 2.196, Val took 62.093s
Iter 400: Train loss 2.314, Learning Rate 1.000e-04, It/sec 0.186, Tokens/sec 112.896, Trained Tokens 187128, Peak mem 8.966 GB
Iter 400: Saved adapter weights to adapters/mindmate_llama32_3b_lora/adapters.safetensors and adapters/mindmate_llama32_3b_lora/0000400_adapters.safetensors.
Iter 410: Train loss 2.245, Learning Rate 1.000e-04, It/sec 0.223, Tokens/sec 108.452, Trained Tokens 191984, Peak mem 8.966 GB
Iter 420: Train loss 2.407, Learning Rate 1.000e-04, It/sec 0.183, Tokens/sec 105.103, Trained Tokens 197736, Peak mem 8.966 GB
Iter 430: Train loss 2.117, Learning Rate 1.000e-04, It/sec 0.220, Tokens/sec 111.604, Trained Tokens 202805, Peak mem 8.966 GB
Iter 440: Train loss 2.064, Learning Rate 1.000e-04, It/sec 0.222, Tokens/sec 102.249, Trained Tokens 207404, Peak mem 8.966 GB
Iter 450: Train loss 2.246, Learning Rate 1.000e-04, It/sec 0.302, Tokens/sec 111.868, Trained Tokens 211114, Peak mem 8.966 GB
Iter 460: Train loss 2.212, Learning Rate 1.000e-04, It/sec 0.237, Tokens/sec 103.127, Trained Tokens 215465, Peak mem 8.966 GB
Iter 470: Train loss 2.117, Learning Rate 1.000e-04, It/sec 0.283, Tokens/sec 109.490, Trained Tokens 219332, Peak mem 8.966 GB
Iter 480: Train loss 2.309, Learning Rate 1.000e-04, It/sec 0.234, Tokens/sec 107.181, Trained Tokens 223906, Peak mem 8.966 GB
[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2275 will be truncated to 2048. Consider pre-splitting your data to save memory.
Iter 490: Train loss 2.266, Learning Rate 1.000e-04, It/sec 0.201, Tokens/sec 104.670, Trained Tokens 229124, Peak mem 9.642 GB
Iter 500: Train loss 2.411, Learning Rate 1.000e-04, It/sec 0.342, Tokens/sec 112.131, Trained Tokens 232407, Peak mem 9.642 GB
Iter 510: Train loss 2.244, Learning Rate 1.000e-04, It/sec 0.241, Tokens/sec 104.377, Trained Tokens 236740, Peak mem 9.642 GB
Iter 520: Train loss 2.214, Learning Rate 1.000e-04, It/sec 0.364, Tokens/sec 159.651, Trained Tokens 241121, Peak mem 9.642 GB
Iter 530: Train loss 2.180, Learning Rate 1.000e-04, It/sec 0.410, Tokens/sec 254.641, Trained Tokens 247331, Peak mem 9.642 GB
Iter 540: Train loss 2.375, Learning Rate 1.000e-04, It/sec 0.423, Tokens/sec 252.109, Trained Tokens 253286, Peak mem 9.642 GB
Iter 550: Train loss 2.224, Learning Rate 1.000e-04, It/sec 0.616, Tokens/sec 251.964, Trained Tokens 257379, Peak mem 9.642 GB
Iter 560: Train loss 2.276, Learning Rate 1.000e-04, It/sec 0.516, Tokens/sec 240.968, Trained Tokens 262048, Peak mem 9.642 GB
Iter 570: Train loss 2.174, Learning Rate 1.000e-04, It/sec 0.616, Tokens/sec 241.441, Trained Tokens 265966, Peak mem 9.642 GB
Iter 580: Train loss 2.345, Learning Rate 1.000e-04, It/sec 0.420, Tokens/sec 223.049, Trained Tokens 271283, Peak mem 9.642 GB
Iter 590: Train loss 2.310, Learning Rate 1.000e-04, It/sec 0.382, Tokens/sec 212.386, Trained Tokens 276840, Peak mem 9.642 GB
Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]Calculating loss...:   4%|▍         | 1/25 [00:00<00:21,  1.14it/s]Calculating loss...:   8%|▊         | 2/25 [00:01<00:15,  1.47it/s]Calculating loss...:  12%|█▏        | 3/25 [00:03<00:27,  1.26s/it]Calculating loss...:  16%|█▌        | 4/25 [00:04<00:29,  1.39s/it]Calculating loss...:  20%|██        | 5/25 [00:06<00:31,  1.58s/it]Calculating loss...:  24%|██▍       | 6/25 [00:07<00:23,  1.22s/it]Calculating loss...:  28%|██▊       | 7/25 [00:08<00:18,  1.05s/it]Calculating loss...:  32%|███▏      | 8/25 [00:10<00:23,  1.41s/it]Calculating loss...:  36%|███▌      | 9/25 [00:11<00:22,  1.42s/it]Calculating loss...:  40%|████      | 10/25 [00:14<00:25,  1.69s/it]Calculating loss...:  44%|████▍     | 11/25 [00:15<00:22,  1.61s/it]Calculating loss...:  48%|████▊     | 12/25 [00:17<00:21,  1.66s/it]Calculating loss...:  52%|█████▏    | 13/25 [00:20<00:25,  2.15s/it]Calculating loss...:  56%|█████▌    | 14/25 [00:21<00:21,  1.92s/it]Calculating loss...:  60%|██████    | 15/25 [00:24<00:21,  2.15s/it]Calculating loss...:  64%|██████▍   | 16/25 [00:27<00:20,  2.31s/it]Calculating loss...:  68%|██████▊   | 17/25 [00:28<00:14,  1.87s/it]Calculating loss...:  72%|███████▏  | 18/25 [00:30<00:13,  1.92s/it]Calculating loss...:  76%|███████▌  | 19/25 [00:32<00:11,  1.99s/it]Calculating loss...:  80%|████████  | 20/25 [00:32<00:07,  1.50s/it]Calculating loss...:  84%|████████▍ | 21/25 [00:33<00:05,  1.31s/it]Calculating loss...:  88%|████████▊ | 22/25 [00:36<00:05,  1.72s/it]Calculating loss...:  92%|█████████▏| 23/25 [00:37<00:03,  1.67s/it]Calculating loss...:  96%|█████████▌| 24/25 [00:39<00:01,  1.75s/it]Calculating loss...: 100%|██████████| 25/25 [00:42<00:00,  1.94s/it]Calculating loss...: 100%|██████████| 25/25 [00:42<00:00,  1.68s/it]
Iter 600: Val loss 2.105, Val took 42.056s
Iter 600: Train loss 2.354, Learning Rate 1.000e-04, It/sec 0.349, Tokens/sec 200.619, Trained Tokens 282584, Peak mem 9.642 GB
Iter 600: Saved adapter weights to adapters/mindmate_llama32_3b_lora/adapters.safetensors and adapters/mindmate_llama32_3b_lora/0000600_adapters.safetensors.
Iter 610: Train loss 2.232, Learning Rate 1.000e-04, It/sec 0.885, Tokens/sec 194.760, Trained Tokens 284785, Peak mem 9.642 GB
Iter 620: Train loss 2.247, Learning Rate 1.000e-04, It/sec 0.525, Tokens/sec 195.922, Trained Tokens 288515, Peak mem 9.642 GB
Iter 630: Train loss 2.303, Learning Rate 1.000e-04, It/sec 0.466, Tokens/sec 188.779, Trained Tokens 292564, Peak mem 9.642 GB
Iter 640: Train loss 2.366, Learning Rate 1.000e-04, It/sec 0.386, Tokens/sec 154.997, Trained Tokens 296576, Peak mem 9.642 GB
Iter 650: Train loss 2.295, Learning Rate 1.000e-04, It/sec 0.338, Tokens/sec 199.780, Trained Tokens 302480, Peak mem 9.642 GB
Iter 660: Train loss 2.248, Learning Rate 1.000e-04, It/sec 0.326, Tokens/sec 196.816, Trained Tokens 308511, Peak mem 9.642 GB
Iter 670: Train loss 2.279, Learning Rate 1.000e-04, It/sec 0.368, Tokens/sec 202.296, Trained Tokens 314013, Peak mem 9.642 GB
Iter 680: Train loss 2.238, Learning Rate 1.000e-04, It/sec 0.319, Tokens/sec 195.338, Trained Tokens 320139, Peak mem 9.642 GB
Iter 690: Train loss 2.171, Learning Rate 1.000e-04, It/sec 0.370, Tokens/sec 199.096, Trained Tokens 325520, Peak mem 9.642 GB
Iter 700: Train loss 2.154, Learning Rate 1.000e-04, It/sec 0.476, Tokens/sec 201.989, Trained Tokens 329764, Peak mem 9.642 GB
Iter 710: Train loss 2.289, Learning Rate 1.000e-04, It/sec 0.384, Tokens/sec 201.268, Trained Tokens 335006, Peak mem 9.642 GB
Iter 720: Train loss 2.355, Learning Rate 1.000e-04, It/sec 0.497, Tokens/sec 195.428, Trained Tokens 338941, Peak mem 9.642 GB
Iter 730: Train loss 2.336, Learning Rate 1.000e-04, It/sec 0.619, Tokens/sec 194.515, Trained Tokens 342081, Peak mem 9.642 GB
Iter 740: Train loss 2.328, Learning Rate 1.000e-04, It/sec 0.482, Tokens/sec 194.546, Trained Tokens 346118, Peak mem 9.642 GB
Iter 750: Train loss 2.267, Learning Rate 1.000e-04, It/sec 0.365, Tokens/sec 153.430, Trained Tokens 350316, Peak mem 9.642 GB
Iter 760: Train loss 2.265, Learning Rate 1.000e-04, It/sec 0.433, Tokens/sec 159.436, Trained Tokens 353996, Peak mem 9.642 GB
Iter 770: Train loss 2.429, Learning Rate 1.000e-04, It/sec 0.413, Tokens/sec 186.552, Trained Tokens 358514, Peak mem 9.642 GB
Iter 780: Train loss 2.252, Learning Rate 1.000e-04, It/sec 0.318, Tokens/sec 177.825, Trained Tokens 364111, Peak mem 9.642 GB
Iter 790: Train loss 2.211, Learning Rate 1.000e-04, It/sec 0.373, Tokens/sec 192.152, Trained Tokens 369264, Peak mem 9.642 GB
Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]Calculating loss...:   4%|▍         | 1/25 [00:02<01:06,  2.78s/it]Calculating loss...:   8%|▊         | 2/25 [00:03<00:38,  1.67s/it]Calculating loss...:  12%|█▏        | 3/25 [00:04<00:32,  1.50s/it]Calculating loss...:  16%|█▌        | 4/25 [00:05<00:23,  1.12s/it]Calculating loss...:  20%|██        | 5/25 [00:06<00:19,  1.03it/s]Calculating loss...:  24%|██▍       | 6/25 [00:07<00:19,  1.01s/it]Calculating loss...:  28%|██▊       | 7/25 [00:08<00:19,  1.10s/it]Calculating loss...:  32%|███▏      | 8/25 [00:09<00:17,  1.02s/it]Calculating loss...:  36%|███▌      | 9/25 [00:13<00:31,  2.00s/it]Calculating loss...:  40%|████      | 10/25 [00:16<00:35,  2.34s/it]Calculating loss...:  44%|████▍     | 11/25 [00:18<00:30,  2.18s/it]Calculating loss...:  48%|████▊     | 12/25 [00:19<00:23,  1.77s/it]Calculating loss...:  52%|█████▏    | 13/25 [00:21<00:23,  1.98s/it]Calculating loss...:  56%|█████▌    | 14/25 [00:22<00:16,  1.54s/it]Calculating loss...:  60%|██████    | 15/25 [00:24<00:16,  1.62s/it]Calculating loss...:  64%|██████▍   | 16/25 [00:26<00:16,  1.78s/it]Calculating loss...:  68%|██████▊   | 17/25 [00:27<00:13,  1.68s/it]Calculating loss...:  72%|███████▏  | 18/25 [00:28<00:09,  1.34s/it]Calculating loss...:  76%|███████▌  | 19/25 [00:30<00:09,  1.59s/it]Calculating loss...:  80%|████████  | 20/25 [00:32<00:08,  1.77s/it]Calculating loss...:  84%|████████▍ | 21/25 [00:34<00:07,  1.79s/it]Calculating loss...:  88%|████████▊ | 22/25 [00:36<00:05,  1.95s/it]Calculating loss...:  92%|█████████▏| 23/25 [00:37<00:03,  1.58s/it]Calculating loss...:  96%|█████████▌| 24/25 [00:38<00:01,  1.43s/it]Calculating loss...: 100%|██████████| 25/25 [00:39<00:00,  1.16s/it]Calculating loss...: 100%|██████████| 25/25 [00:39<00:00,  1.57s/it]
Iter 800: Val loss 2.211, Val took 39.158s
Iter 800: Train loss 2.372, Learning Rate 1.000e-04, It/sec 0.468, Tokens/sec 191.789, Trained Tokens 373362, Peak mem 9.642 GB
Iter 800: Saved adapter weights to adapters/mindmate_llama32_3b_lora/adapters.safetensors and adapters/mindmate_llama32_3b_lora/0000800_adapters.safetensors.
Iter 810: Train loss 2.313, Learning Rate 1.000e-04, It/sec 0.452, Tokens/sec 193.190, Trained Tokens 377637, Peak mem 9.642 GB
Iter 820: Train loss 2.141, Learning Rate 1.000e-04, It/sec 0.418, Tokens/sec 196.316, Trained Tokens 382334, Peak mem 9.642 GB
Iter 830: Train loss 2.286, Learning Rate 1.000e-04, It/sec 0.520, Tokens/sec 190.955, Trained Tokens 386004, Peak mem 9.642 GB
Iter 840: Train loss 2.323, Learning Rate 1.000e-04, It/sec 0.355, Tokens/sec 169.249, Trained Tokens 390778, Peak mem 9.642 GB
Iter 850: Train loss 2.249, Learning Rate 1.000e-04, It/sec 0.451, Tokens/sec 175.374, Trained Tokens 394669, Peak mem 9.642 GB
Iter 860: Train loss 2.155, Learning Rate 1.000e-04, It/sec 0.380, Tokens/sec 190.225, Trained Tokens 399679, Peak mem 9.642 GB
Iter 870: Train loss 2.258, Learning Rate 1.000e-04, It/sec 0.574, Tokens/sec 193.079, Trained Tokens 403041, Peak mem 9.642 GB
Iter 880: Train loss 2.391, Learning Rate 1.000e-04, It/sec 0.303, Tokens/sec 174.127, Trained Tokens 408793, Peak mem 9.642 GB
Iter 890: Train loss 2.366, Learning Rate 1.000e-04, It/sec 0.453, Tokens/sec 185.568, Trained Tokens 412893, Peak mem 9.642 GB
Iter 900: Train loss 2.349, Learning Rate 1.000e-04, It/sec 0.551, Tokens/sec 180.327, Trained Tokens 416167, Peak mem 9.642 GB
Iter 910: Train loss 2.330, Learning Rate 1.000e-04, It/sec 0.376, Tokens/sec 189.019, Trained Tokens 421195, Peak mem 9.642 GB
Iter 920: Train loss 2.287, Learning Rate 1.000e-04, It/sec 0.452, Tokens/sec 190.852, Trained Tokens 425415, Peak mem 9.642 GB
Iter 930: Train loss 2.208, Learning Rate 1.000e-04, It/sec 0.322, Tokens/sec 193.456, Trained Tokens 431426, Peak mem 9.642 GB
Iter 940: Train loss 2.109, Learning Rate 1.000e-04, It/sec 0.728, Tokens/sec 188.593, Trained Tokens 434018, Peak mem 9.642 GB
Iter 950: Train loss 2.312, Learning Rate 1.000e-04, It/sec 0.435, Tokens/sec 190.068, Trained Tokens 438383, Peak mem 9.642 GB
[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2800 will be truncated to 2048. Consider pre-splitting your data to save memory.
Iter 960: Train loss 2.186, Learning Rate 1.000e-04, It/sec 0.339, Tokens/sec 189.212, Trained Tokens 443957, Peak mem 9.642 GB
Iter 970: Train loss 2.341, Learning Rate 1.000e-04, It/sec 0.496, Tokens/sec 184.158, Trained Tokens 447667, Peak mem 9.642 GB
Iter 980: Train loss 2.326, Learning Rate 1.000e-04, It/sec 0.757, Tokens/sec 182.618, Trained Tokens 450080, Peak mem 9.642 GB
Iter 990: Train loss 2.332, Learning Rate 1.000e-04, It/sec 0.321, Tokens/sec 196.277, Trained Tokens 456195, Peak mem 9.642 GB
Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]Calculating loss...:   4%|▍         | 1/25 [00:00<00:22,  1.09it/s]Calculating loss...:   8%|▊         | 2/25 [00:02<00:36,  1.58s/it]Calculating loss...:  12%|█▏        | 3/25 [00:05<00:41,  1.89s/it]Calculating loss...:  16%|█▌        | 4/25 [00:07<00:40,  1.93s/it]Calculating loss...:  20%|██        | 5/25 [00:09<00:40,  2.03s/it]Calculating loss...:  24%|██▍       | 6/25 [00:09<00:29,  1.53s/it]Calculating loss...:  28%|██▊       | 7/25 [00:11<00:27,  1.51s/it]Calculating loss...:  32%|███▏      | 8/25 [00:12<00:21,  1.26s/it]Calculating loss...:  36%|███▌      | 9/25 [00:14<00:25,  1.62s/it]Calculating loss...:  40%|████      | 10/25 [00:16<00:27,  1.80s/it]Calculating loss...:  44%|████▍     | 11/25 [00:17<00:19,  1.42s/it]Calculating loss...:  48%|████▊     | 12/25 [00:18<00:16,  1.27s/it]Calculating loss...:  52%|█████▏    | 13/25 [00:19<00:14,  1.22s/it]Calculating loss...:  56%|█████▌    | 14/25 [00:20<00:11,  1.07s/it]Calculating loss...:  60%|██████    | 15/25 [00:21<00:10,  1.03s/it]Calculating loss...:  64%|██████▍   | 16/25 [00:21<00:08,  1.06it/s]Calculating loss...:  68%|██████▊   | 17/25 [00:24<00:10,  1.33s/it]Calculating loss...:  72%|███████▏  | 18/25 [00:26<00:10,  1.53s/it]Calculating loss...:  76%|███████▌  | 19/25 [00:27<00:09,  1.56s/it]Calculating loss...:  80%|████████  | 20/25 [00:30<00:09,  1.87s/it]Calculating loss...:  84%|████████▍ | 21/25 [00:30<00:05,  1.47s/it]Calculating loss...:  88%|████████▊ | 22/25 [00:32<00:04,  1.64s/it]Calculating loss...:  92%|█████████▏| 23/25 [00:36<00:04,  2.11s/it]Calculating loss...:  96%|█████████▌| 24/25 [00:38<00:02,  2.08s/it]Calculating loss...: 100%|██████████| 25/25 [00:39<00:00,  1.91s/it]Calculating loss...: 100%|██████████| 25/25 [00:39<00:00,  1.58s/it]
Iter 1000: Val loss 2.206, Val took 39.564s
Iter 1000: Train loss 2.069, Learning Rate 1.000e-04, It/sec 0.602, Tokens/sec 192.277, Trained Tokens 459391, Peak mem 9.642 GB
Iter 1000: Saved adapter weights to adapters/mindmate_llama32_3b_lora/adapters.safetensors and adapters/mindmate_llama32_3b_lora/0001000_adapters.safetensors.
Iter 1010: Train loss 2.261, Learning Rate 1.000e-04, It/sec 0.376, Tokens/sec 172.887, Trained Tokens 463989, Peak mem 9.642 GB
Iter 1020: Train loss 2.211, Learning Rate 1.000e-04, It/sec 0.375, Tokens/sec 188.259, Trained Tokens 469015, Peak mem 9.642 GB
Iter 1030: Train loss 2.229, Learning Rate 1.000e-04, It/sec 0.370, Tokens/sec 190.026, Trained Tokens 474145, Peak mem 9.642 GB
Iter 1040: Train loss 2.230, Learning Rate 1.000e-04, It/sec 0.398, Tokens/sec 196.163, Trained Tokens 479072, Peak mem 9.642 GB
Iter 1050: Train loss 2.161, Learning Rate 1.000e-04, It/sec 0.466, Tokens/sec 194.892, Trained Tokens 483250, Peak mem 9.642 GB
Iter 1060: Train loss 2.346, Learning Rate 1.000e-04, It/sec 0.347, Tokens/sec 192.571, Trained Tokens 488806, Peak mem 9.642 GB
Iter 1070: Train loss 2.217, Learning Rate 1.000e-04, It/sec 0.374, Tokens/sec 195.713, Trained Tokens 494034, Peak mem 9.642 GB
Iter 1080: Train loss 2.211, Learning Rate 1.000e-04, It/sec 0.446, Tokens/sec 195.787, Trained Tokens 498425, Peak mem 9.642 GB
Iter 1090: Train loss 2.223, Learning Rate 1.000e-04, It/sec 0.401, Tokens/sec 195.100, Trained Tokens 503286, Peak mem 9.642 GB
Iter 1100: Train loss 2.029, Learning Rate 1.000e-04, It/sec 0.444, Tokens/sec 193.439, Trained Tokens 507647, Peak mem 9.642 GB
Iter 1110: Train loss 2.194, Learning Rate 1.000e-04, It/sec 0.346, Tokens/sec 195.602, Trained Tokens 513306, Peak mem 9.642 GB
Iter 1120: Train loss 2.363, Learning Rate 1.000e-04, It/sec 0.330, Tokens/sec 136.975, Trained Tokens 517462, Peak mem 9.642 GB
Iter 1130: Train loss 2.145, Learning Rate 1.000e-04, It/sec 0.433, Tokens/sec 157.667, Trained Tokens 521106, Peak mem 9.642 GB
Iter 1140: Train loss 2.186, Learning Rate 1.000e-04, It/sec 0.438, Tokens/sec 183.898, Trained Tokens 525301, Peak mem 9.642 GB
Iter 1150: Train loss 2.191, Learning Rate 1.000e-04, It/sec 0.520, Tokens/sec 182.513, Trained Tokens 528811, Peak mem 9.642 GB
Iter 1160: Train loss 2.195, Learning Rate 1.000e-04, It/sec 0.413, Tokens/sec 187.811, Trained Tokens 533357, Peak mem 9.642 GB
Iter 1170: Train loss 2.243, Learning Rate 1.000e-04, It/sec 0.518, Tokens/sec 188.284, Trained Tokens 536995, Peak mem 9.642 GB
Iter 1180: Train loss 2.350, Learning Rate 1.000e-04, It/sec 0.360, Tokens/sec 189.645, Trained Tokens 542258, Peak mem 9.642 GB
Iter 1190: Train loss 2.156, Learning Rate 1.000e-04, It/sec 0.363, Tokens/sec 195.366, Trained Tokens 547640, Peak mem 9.642 GB
Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]Calculating loss...:   4%|▍         | 1/25 [00:02<00:56,  2.37s/it]Calculating loss...:   8%|▊         | 2/25 [00:05<01:02,  2.70s/it]Calculating loss...:  12%|█▏        | 3/25 [00:06<00:41,  1.88s/it]Calculating loss...:  16%|█▌        | 4/25 [00:06<00:29,  1.42s/it]Calculating loss...:  20%|██        | 5/25 [00:09<00:39,  1.98s/it]Calculating loss...:  24%|██▍       | 6/25 [00:10<00:29,  1.55s/it]Calculating loss...:  28%|██▊       | 7/25 [00:13<00:33,  1.88s/it]Calculating loss...:  32%|███▏      | 8/25 [00:13<00:23,  1.40s/it]Calculating loss...:  36%|███▌      | 9/25 [00:15<00:23,  1.47s/it]Calculating loss...:  40%|████      | 10/25 [00:16<00:20,  1.35s/it]Calculating loss...:  44%|████▍     | 11/25 [00:16<00:15,  1.10s/it]Calculating loss...:  48%|████▊     | 12/25 [00:17<00:14,  1.12s/it]Calculating loss...:  52%|█████▏    | 13/25 [00:19<00:14,  1.17s/it]Calculating loss...:  56%|█████▌    | 14/25 [00:19<00:10,  1.02it/s]Calculating loss...:  60%|██████    | 15/25 [00:21<00:12,  1.23s/it]Calculating loss...:  64%|██████▍   | 16/25 [00:22<00:09,  1.08s/it]Calculating loss...:  68%|██████▊   | 17/25 [00:23<00:08,  1.08s/it]Calculating loss...:  72%|███████▏  | 18/25 [00:25<00:09,  1.30s/it]Calculating loss...:  76%|███████▌  | 19/25 [00:26<00:08,  1.45s/it]Calculating loss...:  80%|████████  | 20/25 [00:27<00:05,  1.18s/it]Calculating loss...:  84%|████████▍ | 21/25 [00:28<00:04,  1.25s/it]Calculating loss...:  88%|████████▊ | 22/25 [00:30<00:04,  1.42s/it]Calculating loss...:  92%|█████████▏| 23/25 [00:32<00:02,  1.48s/it]Calculating loss...:  96%|█████████▌| 24/25 [00:32<00:01,  1.15s/it]Calculating loss...: 100%|██████████| 25/25 [00:34<00:00,  1.24s/it]Calculating loss...: 100%|██████████| 25/25 [00:34<00:00,  1.37s/it]
Iter 1200: Val loss 2.226, Val took 34.220s
Iter 1200: Train loss 2.210, Learning Rate 1.000e-04, It/sec 0.537, Tokens/sec 198.745, Trained Tokens 551344, Peak mem 9.642 GB
Iter 1200: Saved adapter weights to adapters/mindmate_llama32_3b_lora/adapters.safetensors and adapters/mindmate_llama32_3b_lora/0001200_adapters.safetensors.
Iter 1210: Train loss 2.370, Learning Rate 1.000e-04, It/sec 0.674, Tokens/sec 184.013, Trained Tokens 554074, Peak mem 9.642 GB
Iter 1220: Train loss 2.285, Learning Rate 1.000e-04, It/sec 0.511, Tokens/sec 190.850, Trained Tokens 557806, Peak mem 9.642 GB
Iter 1230: Train loss 2.200, Learning Rate 1.000e-04, It/sec 0.324, Tokens/sec 188.430, Trained Tokens 563623, Peak mem 9.642 GB
Iter 1240: Train loss 2.299, Learning Rate 1.000e-04, It/sec 0.495, Tokens/sec 192.155, Trained Tokens 567504, Peak mem 9.642 GB
Iter 1250: Train loss 2.157, Learning Rate 1.000e-04, It/sec 0.474, Tokens/sec 194.496, Trained Tokens 571603, Peak mem 9.642 GB
Iter 1260: Train loss 2.261, Learning Rate 1.000e-04, It/sec 0.367, Tokens/sec 191.630, Trained Tokens 576821, Peak mem 9.642 GB
Iter 1270: Train loss 2.361, Learning Rate 1.000e-04, It/sec 0.427, Tokens/sec 194.322, Trained Tokens 581369, Peak mem 9.642 GB
Iter 1280: Train loss 2.199, Learning Rate 1.000e-04, It/sec 0.554, Tokens/sec 192.845, Trained Tokens 584850, Peak mem 9.642 GB
Iter 1290: Train loss 2.311, Learning Rate 1.000e-04, It/sec 0.506, Tokens/sec 191.446, Trained Tokens 588634, Peak mem 9.642 GB
Iter 1300: Train loss 2.397, Learning Rate 1.000e-04, It/sec 0.391, Tokens/sec 193.063, Trained Tokens 593569, Peak mem 9.642 GB
Iter 1310: Train loss 2.329, Learning Rate 1.000e-04, It/sec 0.466, Tokens/sec 190.196, Trained Tokens 597651, Peak mem 9.642 GB
Iter 1320: Train loss 2.308, Learning Rate 1.000e-04, It/sec 0.427, Tokens/sec 188.190, Trained Tokens 602054, Peak mem 9.642 GB
Iter 1330: Train loss 2.171, Learning Rate 1.000e-04, It/sec 0.399, Tokens/sec 187.191, Trained Tokens 606741, Peak mem 9.642 GB
Iter 1340: Train loss 2.166, Learning Rate 1.000e-04, It/sec 0.316, Tokens/sec 176.286, Trained Tokens 612313, Peak mem 9.642 GB
Iter 1350: Train loss 2.302, Learning Rate 1.000e-04, It/sec 0.437, Tokens/sec 179.212, Trained Tokens 616417, Peak mem 9.642 GB
Iter 1360: Train loss 2.244, Learning Rate 1.000e-04, It/sec 0.596, Tokens/sec 177.374, Trained Tokens 619395, Peak mem 9.642 GB
Iter 1370: Train loss 2.181, Learning Rate 1.000e-04, It/sec 0.383, Tokens/sec 178.611, Trained Tokens 624061, Peak mem 9.642 GB
Iter 1380: Train loss 2.100, Learning Rate 1.000e-04, It/sec 0.493, Tokens/sec 186.382, Trained Tokens 627840, Peak mem 9.642 GB
Iter 1390: Train loss 2.062, Learning Rate 1.000e-04, It/sec 0.413, Tokens/sec 167.174, Trained Tokens 631886, Peak mem 9.642 GB
Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]Calculating loss...:   4%|▍         | 1/25 [00:03<01:19,  3.31s/it]Calculating loss...:   8%|▊         | 2/25 [00:05<01:02,  2.71s/it]Calculating loss...:  12%|█▏        | 3/25 [00:07<00:53,  2.43s/it]Calculating loss...:  16%|█▌        | 4/25 [00:09<00:43,  2.07s/it]Calculating loss...:  20%|██        | 5/25 [00:11<00:45,  2.29s/it]Calculating loss...:  24%|██▍       | 6/25 [00:13<00:38,  2.02s/it]Calculating loss...:  28%|██▊       | 7/25 [00:15<00:34,  1.91s/it]Calculating loss...:  32%|███▏      | 8/25 [00:17<00:36,  2.16s/it]Calculating loss...:  36%|███▌      | 9/25 [00:18<00:28,  1.79s/it]Calculating loss...:  40%|████      | 10/25 [00:20<00:27,  1.86s/it]Calculating loss...:  44%|████▍     | 11/25 [00:22<00:23,  1.70s/it]Calculating loss...:  48%|████▊     | 12/25 [00:22<00:16,  1.29s/it]Calculating loss...:  52%|█████▏    | 13/25 [00:24<00:18,  1.54s/it]Calculating loss...:  56%|█████▌    | 14/25 [00:27<00:20,  1.90s/it]Calculating loss...:  60%|██████    | 15/25 [00:28<00:16,  1.61s/it]Calculating loss...:  64%|██████▍   | 16/25 [00:30<00:15,  1.76s/it]Calculating loss...:  68%|██████▊   | 17/25 [00:31<00:11,  1.45s/it]Calculating loss...:  72%|███████▏  | 18/25 [00:31<00:08,  1.17s/it]Calculating loss...:  76%|███████▌  | 19/25 [00:32<00:06,  1.11s/it]Calculating loss...:  80%|████████  | 20/25 [00:33<00:04,  1.00it/s]Calculating loss...:  84%|████████▍ | 21/25 [00:34<00:03,  1.09it/s]Calculating loss...:  88%|████████▊ | 22/25 [00:36<00:03,  1.23s/it]Calculating loss...:  92%|█████████▏| 23/25 [00:36<00:01,  1.03it/s]Calculating loss...:  96%|█████████▌| 24/25 [00:39<00:01,  1.49s/it]Calculating loss...: 100%|██████████| 25/25 [00:43<00:00,  2.28s/it]Calculating loss...: 100%|██████████| 25/25 [00:43<00:00,  1.73s/it]
Iter 1400: Val loss 2.116, Val took 43.203s
Iter 1400: Train loss 2.200, Learning Rate 1.000e-04, It/sec 0.407, Tokens/sec 182.729, Trained Tokens 636373, Peak mem 9.642 GB
Iter 1400: Saved adapter weights to adapters/mindmate_llama32_3b_lora/adapters.safetensors and adapters/mindmate_llama32_3b_lora/0001400_adapters.safetensors.
Iter 1410: Train loss 2.320, Learning Rate 1.000e-04, It/sec 0.309, Tokens/sec 169.485, Trained Tokens 641853, Peak mem 9.642 GB
Iter 1420: Train loss 2.046, Learning Rate 1.000e-04, It/sec 0.346, Tokens/sec 165.643, Trained Tokens 646638, Peak mem 9.642 GB
Iter 1430: Train loss 2.164, Learning Rate 1.000e-04, It/sec 0.404, Tokens/sec 175.799, Trained Tokens 650990, Peak mem 9.642 GB
Iter 1440: Train loss 2.139, Learning Rate 1.000e-04, It/sec 0.421, Tokens/sec 176.379, Trained Tokens 655176, Peak mem 9.642 GB
Iter 1450: Train loss 2.113, Learning Rate 1.000e-04, It/sec 0.431, Tokens/sec 161.484, Trained Tokens 658923, Peak mem 9.642 GB
Iter 1460: Train loss 2.209, Learning Rate 1.000e-04, It/sec 0.333, Tokens/sec 145.910, Trained Tokens 663305, Peak mem 9.642 GB
Iter 1470: Train loss 2.179, Learning Rate 1.000e-04, It/sec 0.442, Tokens/sec 170.461, Trained Tokens 667161, Peak mem 9.642 GB
Iter 1480: Train loss 2.182, Learning Rate 1.000e-04, It/sec 0.251, Tokens/sec 112.098, Trained Tokens 671628, Peak mem 9.642 GB
Iter 1490: Train loss 2.315, Learning Rate 1.000e-04, It/sec 0.258, Tokens/sec 113.413, Trained Tokens 676023, Peak mem 9.642 GB
Iter 1500: Train loss 2.303, Learning Rate 1.000e-04, It/sec 0.329, Tokens/sec 113.163, Trained Tokens 679467, Peak mem 9.642 GB
Iter 1510: Train loss 2.422, Learning Rate 1.000e-04, It/sec 0.196, Tokens/sec 113.197, Trained Tokens 685252, Peak mem 9.642 GB
Iter 1520: Train loss 2.313, Learning Rate 1.000e-04, It/sec 0.322, Tokens/sec 112.143, Trained Tokens 688731, Peak mem 9.642 GB
Iter 1530: Train loss 2.240, Learning Rate 1.000e-04, It/sec 0.295, Tokens/sec 113.147, Trained Tokens 692564, Peak mem 9.642 GB
Iter 1540: Train loss 2.231, Learning Rate 1.000e-04, It/sec 0.269, Tokens/sec 115.037, Trained Tokens 696845, Peak mem 9.642 GB
Iter 1550: Train loss 2.284, Learning Rate 1.000e-04, It/sec 0.332, Tokens/sec 113.578, Trained Tokens 700263, Peak mem 9.642 GB
Iter 1560: Train loss 2.144, Learning Rate 1.000e-04, It/sec 0.306, Tokens/sec 116.095, Trained Tokens 704056, Peak mem 9.642 GB
[WARNING] Some sequences are longer than 2048 tokens. The longest sentence 2368 will be truncated to 2048. Consider pre-splitting your data to save memory.
Iter 1570: Train loss 2.179, Learning Rate 1.000e-04, It/sec 0.145, Tokens/sec 111.824, Trained Tokens 711743, Peak mem 9.642 GB
Iter 1580: Train loss 2.265, Learning Rate 1.000e-04, It/sec 0.203, Tokens/sec 114.001, Trained Tokens 717352, Peak mem 9.642 GB
Iter 1590: Train loss 2.032, Learning Rate 1.000e-04, It/sec 0.287, Tokens/sec 113.889, Trained Tokens 721315, Peak mem 9.642 GB
Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]Calculating loss...:   4%|▍         | 1/25 [00:02<01:05,  2.72s/it]Calculating loss...:   8%|▊         | 2/25 [00:04<00:54,  2.37s/it]Calculating loss...:  12%|█▏        | 3/25 [00:07<00:55,  2.53s/it]Calculating loss...:  16%|█▌        | 4/25 [00:08<00:42,  2.01s/it]Calculating loss...:  20%|██        | 5/25 [00:09<00:32,  1.62s/it]Calculating loss...:  24%|██▍       | 6/25 [00:13<00:43,  2.28s/it]Calculating loss...:  28%|██▊       | 7/25 [00:15<00:38,  2.12s/it]Calculating loss...:  32%|███▏      | 8/25 [00:21<01:02,  3.65s/it]Calculating loss...:  36%|███▌      | 9/25 [00:24<00:53,  3.35s/it]Calculating loss...:  40%|████      | 10/25 [00:26<00:44,  2.97s/it]Calculating loss...:  44%|████▍     | 11/25 [00:28<00:34,  2.44s/it]Calculating loss...:  48%|████▊     | 12/25 [00:30<00:31,  2.43s/it]Calculating loss...:  52%|█████▏    | 13/25 [00:33<00:29,  2.49s/it]Calculating loss...:  56%|█████▌    | 14/25 [00:33<00:22,  2.01s/it]Calculating loss...:  60%|██████    | 15/25 [00:37<00:23,  2.32s/it]Calculating loss...:  64%|██████▍   | 16/25 [00:41<00:25,  2.83s/it]Calculating loss...:  68%|██████▊   | 17/25 [00:45<00:25,  3.23s/it]Calculating loss...:  72%|███████▏  | 18/25 [00:47<00:20,  2.97s/it]Calculating loss...:  76%|███████▌  | 19/25 [00:50<00:18,  3.08s/it]Calculating loss...:  80%|████████  | 20/25 [00:52<00:12,  2.52s/it]Calculating loss...:  84%|████████▍ | 21/25 [00:53<00:08,  2.05s/it]Calculating loss...:  88%|████████▊ | 22/25 [00:56<00:07,  2.34s/it]Calculating loss...:  92%|█████████▏| 23/25 [01:00<00:06,  3.03s/it]Calculating loss...:  96%|█████████▌| 24/25 [01:04<00:03,  3.23s/it]Calculating loss...: 100%|██████████| 25/25 [01:05<00:00,  2.45s/it]Calculating loss...: 100%|██████████| 25/25 [01:05<00:00,  2.60s/it]
Iter 1600: Val loss 2.251, Val took 65.063s
Iter 1600: Train loss 2.363, Learning Rate 1.000e-04, It/sec 0.248, Tokens/sec 114.367, Trained Tokens 725922, Peak mem 9.642 GB
Iter 1600: Saved adapter weights to adapters/mindmate_llama32_3b_lora/adapters.safetensors and adapters/mindmate_llama32_3b_lora/0001600_adapters.safetensors.
Iter 1610: Train loss 2.108, Learning Rate 1.000e-04, It/sec 0.385, Tokens/sec 107.990, Trained Tokens 728730, Peak mem 9.642 GB
Iter 1620: Train loss 2.233, Learning Rate 1.000e-04, It/sec 0.175, Tokens/sec 108.268, Trained Tokens 734921, Peak mem 9.642 GB
Iter 1630: Train loss 2.273, Learning Rate 1.000e-04, It/sec 0.252, Tokens/sec 117.450, Trained Tokens 739588, Peak mem 9.642 GB
Iter 1640: Train loss 2.316, Learning Rate 1.000e-04, It/sec 0.277, Tokens/sec 116.479, Trained Tokens 743794, Peak mem 9.642 GB
Iter 1650: Train loss 2.158, Learning Rate 1.000e-04, It/sec 0.303, Tokens/sec 113.603, Trained Tokens 747549, Peak mem 9.642 GB
Iter 1660: Train loss 2.283, Learning Rate 1.000e-04, It/sec 0.188, Tokens/sec 117.456, Trained Tokens 753806, Peak mem 9.642 GB
Iter 1670: Train loss 2.144, Learning Rate 1.000e-04, It/sec 0.255, Tokens/sec 118.145, Trained Tokens 758435, Peak mem 9.642 GB
Iter 1680: Train loss 2.194, Learning Rate 1.000e-04, It/sec 0.352, Tokens/sec 112.124, Trained Tokens 761618, Peak mem 9.642 GB
Iter 1690: Train loss 2.303, Learning Rate 1.000e-04, It/sec 0.324, Tokens/sec 118.792, Trained Tokens 765288, Peak mem 9.642 GB
Iter 1700: Train loss 2.336, Learning Rate 1.000e-04, It/sec 0.296, Tokens/sec 112.396, Trained Tokens 769080, Peak mem 9.642 GB
Iter 1710: Train loss 2.094, Learning Rate 1.000e-04, It/sec 0.299, Tokens/sec 113.006, Trained Tokens 772857, Peak mem 9.642 GB
Iter 1720: Train loss 2.348, Learning Rate 1.000e-04, It/sec 0.464, Tokens/sec 114.003, Trained Tokens 775313, Peak mem 9.642 GB
Iter 1730: Train loss 2.097, Learning Rate 1.000e-04, It/sec 0.374, Tokens/sec 116.247, Trained Tokens 778424, Peak mem 9.642 GB
Iter 1740: Train loss 2.038, Learning Rate 1.000e-04, It/sec 0.195, Tokens/sec 116.324, Trained Tokens 784389, Peak mem 9.642 GB
Iter 1750: Train loss 2.242, Learning Rate 1.000e-04, It/sec 0.306, Tokens/sec 119.883, Trained Tokens 788304, Peak mem 9.642 GB
Iter 1760: Train loss 2.349, Learning Rate 1.000e-04, It/sec 0.317, Tokens/sec 117.974, Trained Tokens 792025, Peak mem 9.642 GB
Iter 1770: Train loss 2.314, Learning Rate 1.000e-04, It/sec 0.281, Tokens/sec 115.656, Trained Tokens 796139, Peak mem 9.642 GB
Iter 1780: Train loss 1.931, Learning Rate 1.000e-04, It/sec 0.253, Tokens/sec 116.449, Trained Tokens 800742, Peak mem 9.642 GB
Iter 1790: Train loss 1.961, Learning Rate 1.000e-04, It/sec 0.210, Tokens/sec 114.878, Trained Tokens 806218, Peak mem 9.642 GB
Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]Calculating loss...:   4%|▍         | 1/25 [00:03<01:24,  3.50s/it]Calculating loss...:   8%|▊         | 2/25 [00:05<01:04,  2.81s/it]Calculating loss...:  12%|█▏        | 3/25 [00:10<01:18,  3.55s/it]Calculating loss...:  16%|█▌        | 4/25 [00:14<01:16,  3.66s/it]Calculating loss...:  20%|██        | 5/25 [00:15<00:59,  3.00s/it]Calculating loss...:  24%|██▍       | 6/25 [00:17<00:45,  2.39s/it]Calculating loss...:  28%|██▊       | 7/25 [00:19<00:42,  2.38s/it]Calculating loss...:  32%|███▏      | 8/25 [00:20<00:34,  2.01s/it]Calculating loss...:  36%|███▌      | 9/25 [00:24<00:43,  2.69s/it]Calculating loss...:  40%|████      | 10/25 [00:31<00:57,  3.84s/it]Calculating loss...:  44%|████▍     | 11/25 [00:31<00:39,  2.84s/it]Calculating loss...:  48%|████▊     | 12/25 [00:34<00:34,  2.69s/it]Calculating loss...:  52%|█████▏    | 13/25 [00:37<00:35,  2.94s/it]Calculating loss...:  56%|█████▌    | 14/25 [00:38<00:26,  2.40s/it]Calculating loss...:  60%|██████    | 15/25 [00:41<00:23,  2.38s/it]Calculating loss...:  64%|██████▍   | 16/25 [00:44<00:23,  2.64s/it]Calculating loss...:  68%|██████▊   | 17/25 [00:47<00:22,  2.78s/it]Calculating loss...:  72%|███████▏  | 18/25 [00:48<00:15,  2.21s/it]Calculating loss...:  76%|███████▌  | 19/25 [00:53<00:18,  3.10s/it]Calculating loss...:  80%|████████  | 20/25 [00:54<00:12,  2.52s/it]Calculating loss...:  84%|████████▍ | 21/25 [00:57<00:10,  2.65s/it]Calculating loss...:  88%|████████▊ | 22/25 [01:01<00:08,  2.92s/it]Calculating loss...:  92%|█████████▏| 23/25 [01:02<00:04,  2.42s/it]Calculating loss...:  96%|█████████▌| 24/25 [01:03<00:01,  1.97s/it]Calculating loss...: 100%|██████████| 25/25 [01:05<00:00,  1.99s/it]Calculating loss...: 100%|██████████| 25/25 [01:05<00:00,  2.62s/it]
Iter 1800: Val loss 2.174, Val took 65.538s
Iter 1800: Train loss 2.099, Learning Rate 1.000e-04, It/sec 0.186, Tokens/sec 117.736, Trained Tokens 812546, Peak mem 9.642 GB
Iter 1800: Saved adapter weights to adapters/mindmate_llama32_3b_lora/adapters.safetensors and adapters/mindmate_llama32_3b_lora/0001800_adapters.safetensors.
Iter 1810: Train loss 1.868, Learning Rate 1.000e-04, It/sec 0.297, Tokens/sec 113.353, Trained Tokens 816366, Peak mem 9.642 GB
Iter 1820: Train loss 2.100, Learning Rate 1.000e-04, It/sec 0.248, Tokens/sec 113.127, Trained Tokens 820928, Peak mem 9.642 GB
Iter 1830: Train loss 2.035, Learning Rate 1.000e-04, It/sec 0.248, Tokens/sec 111.564, Trained Tokens 825423, Peak mem 9.642 GB
Iter 1840: Train loss 1.882, Learning Rate 1.000e-04, It/sec 0.252, Tokens/sec 113.185, Trained Tokens 829918, Peak mem 9.642 GB
Iter 1850: Train loss 1.803, Learning Rate 1.000e-04, It/sec 0.245, Tokens/sec 118.778, Trained Tokens 834776, Peak mem 9.642 GB
Iter 1860: Train loss 1.996, Learning Rate 1.000e-04, It/sec 0.325, Tokens/sec 117.202, Trained Tokens 838386, Peak mem 9.642 GB
Iter 1870: Train loss 2.059, Learning Rate 1.000e-04, It/sec 0.241, Tokens/sec 119.515, Trained Tokens 843343, Peak mem 9.642 GB
Iter 1880: Train loss 1.974, Learning Rate 1.000e-04, It/sec 0.253, Tokens/sec 119.152, Trained Tokens 848044, Peak mem 9.642 GB
Iter 1890: Train loss 1.902, Learning Rate 1.000e-04, It/sec 0.283, Tokens/sec 119.902, Trained Tokens 852274, Peak mem 9.642 GB
Iter 1900: Train loss 1.736, Learning Rate 1.000e-04, It/sec 0.284, Tokens/sec 120.109, Trained Tokens 856497, Peak mem 9.642 GB
Iter 1910: Train loss 1.941, Learning Rate 1.000e-04, It/sec 0.277, Tokens/sec 115.059, Trained Tokens 860655, Peak mem 9.642 GB
Iter 1920: Train loss 1.912, Learning Rate 1.000e-04, It/sec 0.214, Tokens/sec 117.436, Trained Tokens 866150, Peak mem 9.642 GB
Iter 1930: Train loss 2.033, Learning Rate 1.000e-04, It/sec 0.270, Tokens/sec 117.800, Trained Tokens 870511, Peak mem 9.642 GB
Iter 1940: Train loss 1.944, Learning Rate 1.000e-04, It/sec 0.214, Tokens/sec 116.915, Trained Tokens 875971, Peak mem 9.642 GB
Iter 1950: Train loss 2.054, Learning Rate 1.000e-04, It/sec 0.256, Tokens/sec 117.940, Trained Tokens 880575, Peak mem 9.642 GB
Iter 1960: Train loss 1.838, Learning Rate 1.000e-04, It/sec 0.292, Tokens/sec 116.926, Trained Tokens 884585, Peak mem 9.642 GB
Iter 1970: Train loss 2.073, Learning Rate 1.000e-04, It/sec 0.298, Tokens/sec 120.526, Trained Tokens 888628, Peak mem 9.642 GB
Iter 1980: Train loss 1.891, Learning Rate 1.000e-04, It/sec 0.274, Tokens/sec 116.990, Trained Tokens 892904, Peak mem 9.642 GB
Iter 1990: Train loss 1.930, Learning Rate 1.000e-04, It/sec 0.241, Tokens/sec 118.537, Trained Tokens 897832, Peak mem 9.642 GB
Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]Calculating loss...:   4%|▍         | 1/25 [00:03<01:16,  3.19s/it]Calculating loss...:   8%|▊         | 2/25 [00:06<01:17,  3.37s/it]Calculating loss...:  12%|█▏        | 3/25 [00:09<01:04,  2.93s/it]Calculating loss...:  16%|█▌        | 4/25 [00:10<00:51,  2.46s/it]Calculating loss...:  20%|██        | 5/25 [00:13<00:52,  2.62s/it]Calculating loss...:  24%|██▍       | 6/25 [00:14<00:38,  2.02s/it]Calculating loss...:  28%|██▊       | 7/25 [00:15<00:28,  1.56s/it]Calculating loss...:  32%|███▏      | 8/25 [00:15<00:21,  1.27s/it]Calculating loss...:  36%|███▌      | 9/25 [00:18<00:25,  1.60s/it]Calculating loss...:  40%|████      | 10/25 [00:21<00:33,  2.24s/it]Calculating loss...:  44%|████▍     | 11/25 [00:23<00:26,  1.92s/it]Calculating loss...:  48%|████▊     | 12/25 [00:24<00:22,  1.71s/it]Calculating loss...:  52%|█████▏    | 13/25 [00:27<00:25,  2.10s/it]Calculating loss...:  56%|█████▌    | 14/25 [00:29<00:21,  2.00s/it]Calculating loss...:  60%|██████    | 15/25 [00:33<00:27,  2.78s/it]Calculating loss...:  64%|██████▍   | 16/25 [00:38<00:32,  3.56s/it]Calculating loss...:  68%|██████▊   | 17/25 [00:40<00:23,  2.93s/it]Calculating loss...:  72%|███████▏  | 18/25 [00:43<00:19,  2.84s/it]Calculating loss...:  76%|███████▌  | 19/25 [00:46<00:17,  2.96s/it]Calculating loss...:  80%|████████  | 20/25 [00:46<00:11,  2.25s/it]Calculating loss...:  84%|████████▍ | 21/25 [00:49<00:09,  2.45s/it]Calculating loss...:  88%|████████▊ | 22/25 [00:52<00:07,  2.52s/it]Calculating loss...:  92%|█████████▏| 23/25 [00:57<00:06,  3.20s/it]Calculating loss...:  96%|█████████▌| 24/25 [01:00<00:03,  3.29s/it]Calculating loss...: 100%|██████████| 25/25 [01:03<00:00,  3.09s/it]Calculating loss...: 100%|██████████| 25/25 [01:03<00:00,  2.54s/it]
Iter 2000: Val loss 2.138, Val took 63.477s
Iter 2000: Train loss 1.764, Learning Rate 1.000e-04, It/sec 0.308, Tokens/sec 119.844, Trained Tokens 901728, Peak mem 9.642 GB
Iter 2000: Saved adapter weights to adapters/mindmate_llama32_3b_lora/adapters.safetensors and adapters/mindmate_llama32_3b_lora/0002000_adapters.safetensors.
Iter 2010: Train loss 2.101, Learning Rate 1.000e-04, It/sec 0.225, Tokens/sec 116.721, Trained Tokens 906921, Peak mem 9.642 GB
Iter 2020: Train loss 1.936, Learning Rate 1.000e-04, It/sec 0.262, Tokens/sec 118.640, Trained Tokens 911442, Peak mem 9.642 GB
Iter 2030: Train loss 1.867, Learning Rate 1.000e-04, It/sec 0.291, Tokens/sec 117.701, Trained Tokens 915482, Peak mem 9.642 GB
Iter 2040: Train loss 2.141, Learning Rate 1.000e-04, It/sec 0.302, Tokens/sec 120.523, Trained Tokens 919469, Peak mem 9.642 GB
Iter 2050: Train loss 2.071, Learning Rate 1.000e-04, It/sec 0.308, Tokens/sec 120.548, Trained Tokens 923382, Peak mem 9.642 GB
Iter 2060: Train loss 2.035, Learning Rate 1.000e-04, It/sec 0.392, Tokens/sec 115.221, Trained Tokens 926325, Peak mem 9.642 GB
Iter 2070: Train loss 1.927, Learning Rate 1.000e-04, It/sec 0.243, Tokens/sec 118.652, Trained Tokens 931199, Peak mem 9.642 GB
Iter 2080: Train loss 1.714, Learning Rate 1.000e-04, It/sec 0.331, Tokens/sec 114.194, Trained Tokens 934648, Peak mem 9.642 GB
Iter 2090: Train loss 1.924, Learning Rate 1.000e-04, It/sec 0.271, Tokens/sec 118.444, Trained Tokens 939025, Peak mem 9.642 GB
Iter 2100: Train loss 1.960, Learning Rate 1.000e-04, It/sec 0.319, Tokens/sec 117.419, Trained Tokens 942706, Peak mem 9.642 GB
Iter 2110: Train loss 2.028, Learning Rate 1.000e-04, It/sec 0.242, Tokens/sec 119.431, Trained Tokens 947648, Peak mem 9.642 GB
Iter 2120: Train loss 1.817, Learning Rate 1.000e-04, It/sec 0.375, Tokens/sec 117.039, Trained Tokens 950773, Peak mem 9.642 GB
Iter 2130: Train loss 1.914, Learning Rate 1.000e-04, It/sec 0.195, Tokens/sec 119.529, Trained Tokens 956915, Peak mem 9.642 GB
Iter 2140: Train loss 2.087, Learning Rate 1.000e-04, It/sec 0.248, Tokens/sec 122.480, Trained Tokens 961856, Peak mem 9.642 GB
Iter 2150: Train loss 1.924, Learning Rate 1.000e-04, It/sec 0.284, Tokens/sec 118.995, Trained Tokens 966039, Peak mem 9.642 GB
Iter 2160: Train loss 1.998, Learning Rate 1.000e-04, It/sec 0.237, Tokens/sec 117.945, Trained Tokens 971026, Peak mem 9.642 GB
Iter 2170: Train loss 1.971, Learning Rate 1.000e-04, It/sec 0.264, Tokens/sec 121.262, Trained Tokens 975627, Peak mem 9.642 GB
Iter 2180: Train loss 2.021, Learning Rate 1.000e-04, It/sec 0.258, Tokens/sec 120.438, Trained Tokens 980288, Peak mem 9.642 GB
Iter 2190: Train loss 2.008, Learning Rate 1.000e-04, It/sec 0.239, Tokens/sec 119.376, Trained Tokens 985281, Peak mem 9.642 GB
Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]Calculating loss...:   4%|▍         | 1/25 [00:04<01:39,  4.14s/it]Calculating loss...:   8%|▊         | 2/25 [00:06<01:10,  3.07s/it]Calculating loss...:  12%|█▏        | 3/25 [00:09<01:08,  3.13s/it]Calculating loss...:  16%|█▌        | 4/25 [00:12<01:03,  3.04s/it]Calculating loss...:  20%|██        | 5/25 [00:16<01:08,  3.43s/it]Calculating loss...:  24%|██▍       | 6/25 [00:18<00:57,  3.05s/it]Calculating loss...:  28%|██▊       | 7/25 [00:20<00:43,  2.43s/it]Calculating loss...:  32%|███▏      | 8/25 [00:21<00:32,  1.94s/it]Calculating loss...:  36%|███▌      | 9/25 [00:22<00:27,  1.70s/it]Calculating loss...:  40%|████      | 10/25 [00:23<00:23,  1.54s/it]Calculating loss...:  44%|████▍     | 11/25 [00:26<00:27,  1.96s/it]Calculating loss...:  48%|████▊     | 12/25 [00:27<00:23,  1.81s/it]Calculating loss...:  52%|█████▏    | 13/25 [00:28<00:19,  1.62s/it]Calculating loss...:  56%|█████▌    | 14/25 [00:31<00:20,  1.84s/it]Calculating loss...:  60%|██████    | 15/25 [00:32<00:16,  1.63s/it]Calculating loss...:  64%|██████▍   | 16/25 [00:33<00:12,  1.41s/it]Calculating loss...:  68%|██████▊   | 17/25 [00:38<00:19,  2.43s/it]Calculating loss...:  72%|███████▏  | 18/25 [00:41<00:17,  2.56s/it]Calculating loss...:  76%|███████▌  | 19/25 [00:42<00:12,  2.14s/it]Calculating loss...:  80%|████████  | 20/25 [00:42<00:08,  1.68s/it]Calculating loss...:  84%|████████▍ | 21/25 [00:44<00:06,  1.61s/it]Calculating loss...:  88%|████████▊ | 22/25 [00:47<00:06,  2.10s/it]Calculating loss...:  92%|█████████▏| 23/25 [00:51<00:05,  2.70s/it]Calculating loss...:  96%|█████████▌| 24/25 [00:54<00:02,  2.85s/it]Calculating loss...: 100%|██████████| 25/25 [00:57<00:00,  2.78s/it]Calculating loss...: 100%|██████████| 25/25 [00:57<00:00,  2.29s/it]
Iter 2200: Val loss 2.153, Val took 57.367s
Iter 2200: Train loss 1.972, Learning Rate 1.000e-04, It/sec 0.216, Tokens/sec 117.616, Trained Tokens 990719, Peak mem 9.642 GB
Iter 2200: Saved adapter weights to adapters/mindmate_llama32_3b_lora/adapters.safetensors and adapters/mindmate_llama32_3b_lora/0002200_adapters.safetensors.
Iter 2210: Train loss 2.010, Learning Rate 1.000e-04, It/sec 0.287, Tokens/sec 116.754, Trained Tokens 994793, Peak mem 9.642 GB
Iter 2220: Train loss 2.113, Learning Rate 1.000e-04, It/sec 0.211, Tokens/sec 99.863, Trained Tokens 999536, Peak mem 9.642 GB
Iter 2230: Train loss 1.933, Learning Rate 1.000e-04, It/sec 0.159, Tokens/sec 99.921, Trained Tokens 1005839, Peak mem 9.642 GB
Iter 2240: Train loss 1.625, Learning Rate 1.000e-04, It/sec 0.413, Tokens/sec 100.823, Trained Tokens 1008283, Peak mem 9.642 GB
